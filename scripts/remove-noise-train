#!/usr/bin/env python3

import glob
import os.path

import cv2
import keras.saving
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import yaml
from keras.layers import Conv2D, Conv2DTranspose, Dropout, Input, MaxPooling2D, UpSampling2D, concatenate
from keras.models import Model

from clean_document.apply import apply
from clean_document.utils import data_augmentation_image

NUMBER_OF_IMAGES = 5
IMAGE_DIMENSION = 256


def model_u_net() -> Model:
    # Create a model based on a pretrained U-net
    # https://keras.io/examples/vision/oxford_pets_image_segmentation/
    # https://keras.io/api/applications/
    base_model = tf.keras.applications.MobileNetV2(input_shape=(None, None, 3), include_top=False)
    # Use the activations of these layers
    layer_names = [
        "block_1_expand_relu",  # 64x64
        "block_3_expand_relu",  # 32x32
        "block_6_expand_relu",  # 16x16
        "block_13_expand_relu",  # 8x8
        "block_16_project",  # 4x4
    ]
    layers = [base_model.get_layer(name).output for name in layer_names]
    # Create the feature extraction model
    down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)
    down_stack.trainable = False


def build_u_net_model(input_layer, start_neurons):
    conv1 = Conv2D(start_neurons * 1, (3, 3), activation="relu", padding="same")(input_layer)
    conv1 = Conv2D(start_neurons * 1, (3, 3), activation="relu", padding="same")(conv1)
    pool1 = MaxPooling2D((2, 2))(conv1)
    pool1 = Dropout(0.25)(pool1)

    conv2 = Conv2D(start_neurons * 2, (3, 3), activation="relu", padding="same")(pool1)
    conv2 = Conv2D(start_neurons * 2, (3, 3), activation="relu", padding="same")(conv2)
    pool2 = MaxPooling2D((2, 2))(conv2)
    pool2 = Dropout(0.5)(pool2)

    conv3 = Conv2D(start_neurons * 4, (3, 3), activation="relu", padding="same")(pool2)
    conv3 = Conv2D(start_neurons * 4, (3, 3), activation="relu", padding="same")(conv3)
    pool3 = MaxPooling2D((2, 2))(conv3)
    pool3 = Dropout(0.5)(pool3)

    conv4 = Conv2D(start_neurons * 8, (3, 3), activation="relu", padding="same")(pool3)
    conv4 = Conv2D(start_neurons * 8, (3, 3), activation="relu", padding="same")(conv4)
    pool4 = MaxPooling2D((2, 2))(conv4)
    pool4 = Dropout(0.5)(pool4)

    # Middle
    convm = Conv2D(start_neurons * 16, (3, 3), activation="relu", padding="same")(pool4)
    convm = Conv2D(start_neurons * 16, (3, 3), activation="relu", padding="same")(convm)

    deconv4 = Conv2DTranspose(start_neurons * 8, (3, 3), strides=(2, 2), padding="same")(convm)
    uconv4 = concatenate([deconv4, conv4])
    uconv4 = Dropout(0.5)(uconv4)
    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation="relu", padding="same")(uconv4)
    uconv4 = Conv2D(start_neurons * 8, (3, 3), activation="relu", padding="same")(uconv4)

    deconv3 = Conv2DTranspose(start_neurons * 4, (3, 3), strides=(2, 2), padding="same")(uconv4)
    uconv3 = concatenate([deconv3, conv3])
    uconv3 = Dropout(0.5)(uconv3)
    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation="relu", padding="same")(uconv3)
    uconv3 = Conv2D(start_neurons * 4, (3, 3), activation="relu", padding="same")(uconv3)

    deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding="same")(uconv3)
    uconv2 = concatenate([deconv2, conv2])
    uconv2 = Dropout(0.5)(uconv2)
    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation="relu", padding="same")(uconv2)
    uconv2 = Conv2D(start_neurons * 2, (3, 3), activation="relu", padding="same")(uconv2)

    deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding="same")(uconv2)
    uconv1 = concatenate([deconv1, conv1])
    uconv1 = Dropout(0.5)(uconv1)
    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation="relu", padding="same")(uconv1)
    uconv1 = Conv2D(start_neurons * 1, (3, 3), activation="relu", padding="same")(uconv1)

    output_layer = Conv2D(1, (1, 1), padding="same", activation="sigmoid")(uconv1)

    return output_layer


def model_u_net_16() -> Model:
    input_layer = Input((None, None, 3))
    return build_u_net_model(input_layer, 16)


def model_u_net_32() -> Model:
    input_layer = Input((None, None, 3))
    return build_u_net_model(input_layer, 32)


def model_u_net_64() -> Model:
    input_layer = Input((None, None, 3))
    return build_u_net_model(input_layer, 64)


def model_32_64():
    input_img = Input(shape=(None, None, 3), name="image_input")

    # Encoder
    x = Conv2D(32, (3, 3), activation="relu", padding="same", name="Conv1")(input_img)
    x = MaxPooling2D((2, 2), padding="same", name="pool1")(x)
    x = Conv2D(64, (3, 3), activation="relu", padding="same", name="Conv2")(x)
    x = MaxPooling2D((2, 2), padding="same", name="pool2")(x)

    # Decoder
    x = Conv2D(64, (3, 3), activation="relu", padding="same", name="Conv3")(x)
    x = UpSampling2D((2, 2), name="upsample1")(x)
    x = Conv2D(32, (3, 3), activation="relu", padding="same", name="Conv4")(x)
    x = UpSampling2D((2, 2), name="upsample2")(x)
    x = Conv2D(3, (3, 3), activation="sigmoid", padding="same", name="Conv5")(x)

    # Model
    autoencoder = Model(inputs=input_img, outputs=x)
    autoencoder.compile(optimizer="adam", loss="binary_crossentropy")

    return autoencoder


def model_64_128() -> Model:
    input_img = Input(shape=(None, None, 3), name="image_input")

    # Encoder
    x = Conv2D(64, (3, 3), activation="relu", padding="same", name="ConvEnc1")(input_img)
    x = MaxPooling2D((2, 2), padding="same", name="pool1")(x)
    x = Conv2D(128, (3, 3), activation="relu", padding="same", name="ConvEnc2")(x)
    x = MaxPooling2D((2, 2), padding="same", name="pool2")(x)

    # Decoder
    x = Conv2D(128, (3, 3), activation="relu", padding="same", name="ConvDec1")(x)
    x = UpSampling2D((2, 2), name="upsample1")(x)
    x = Conv2D(64, (3, 3), activation="relu", padding="same", name="ConvDec2")(x)
    x = UpSampling2D((2, 2), name="upsample2")(x)
    x = Conv2D(3, (3, 3), activation="sigmoid", padding="same", name="ConvDec3")(x)

    # Model
    autoencoder = Model(inputs=input_img, outputs=x)
    autoencoder.compile(optimizer="adam", loss="binary_crossentropy")

    return autoencoder


def model_64_128_256() -> Model:
    input_img = Input(shape=(None, None, 3), name="image_input")

    # Encoder
    x = Conv2D(64, (3, 3), activation="relu", padding="same", name="ConvEnc1")(input_img)
    x = MaxPooling2D((2, 2), padding="same", name="pool1")(x)
    x = Conv2D(128, (3, 3), activation="relu", padding="same", name="ConvEnc2")(x)
    x = MaxPooling2D((2, 2), padding="same", name="pool2")(x)
    x = Conv2D(256, (3, 3), activation="relu", padding="same", name="ConvEnc3")(x)
    x = MaxPooling2D((2, 2), padding="same", name="pool3")(x)

    # Decoder
    x = Conv2D(256, (3, 3), activation="relu", padding="same", name="ConvDec1")(x)
    x = UpSampling2D((2, 2), name="upsample1")(x)
    x = Conv2D(128, (3, 3), activation="relu", padding="same", name="ConvDec2")(x)
    x = UpSampling2D((2, 2), name="upsample2")(x)
    x = Conv2D(64, (3, 3), activation="relu", padding="same", name="ConvDec3")(x)
    x = UpSampling2D((2, 2), name="upsample3")(x)
    x = Conv2D(3, (3, 3), activation="sigmoid", padding="same", name="ConvDec4")(x)

    # Model
    autoencoder = Model(inputs=input_img, outputs=x)
    autoencoder.compile(optimizer="adam", loss="binary_crossentropy")

    return autoencoder


def model():
    input_ = layers.Input(shape=(None, None, 3))

    # Encoder
    x = layers.Conv2D(32, (3, 3), activation="relu", padding="same")(input_)
    x = layers.MaxPooling2D((2, 2), padding="same")(x)
    x = layers.Conv2D(32, (3, 3), activation="relu", padding="same")(x)
    x = layers.MaxPooling2D((2, 2), padding="same")(x)

    # Decoder
    x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation="relu", padding="same")(x)
    x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation="relu", padding="same")(x)
    x = layers.Conv2D(1, (3, 3), activation="sigmoid", padding="same")(x)

    # Autoencoder
    autoencoder = Model(input, x)
    autoencoder.compile(optimizer="adam", loss="binary_crossentropy")
    return autoencoder


MODELS = {
    f"denoise_{IMAGE_DIMENSION}": model,
}


dirty_image_file_name = random.choice(glob.glob("data/dirty/*.png"))
dirty_image = io.imread(dirty_image_file_name)

image_gray = cv2.cvtColor(dirty_image, cv2.COLOR_BGR2GRAY)
# add black band at the image bottom
image_gray = np.vstack((image_gray, np.zeros((50, image_gray.shape[1]))))

image_treshold = image_gray > 200
image_treshold = image_treshold.astype(np.uint8) * 255
# dilate (denoise)
image_treshold = cv2.dilate(image_treshold, np.ones((10, 10), np.uint8), iterations=1)
# erode
image_treshold = cv2.erode(image_treshold, np.ones((50, 50), np.uint8), iterations=1)
# remove the black band
image_treshold = image_treshold[:-50, :]
dirty_image[image_treshold < 127] = [255, 255, 255]


# Generator that returns the images
class Images:
    def __init__(self, filenames: list[str]):
        self.filenames = filenames

    def __call__(self):
        for filename in self.filenames:
            image = io.imread(train_filename)
            width = max(image.shape[0], dirty_image.shape[0])
            height = max(image.shape[1], dirty_image.shape[1])
            if width != image.shape[0]:
                # Add white band at the right
                image = np.hstack(
                    (image, np.ones((image.shape[0], height - image.shape[1], 3), dtype=np.uint8) * 255)
                )
            if height != image.shape[1]:
                # Add white band at the bottom
                image = np.vstack(
                    (image, np.ones((width - image.shape[0], image.shape[1], 3), dtype=np.uint8) * 255)
                )
            dirty_image.copy()
            if width != current_dirty_image.shape[0]:
                # Add white band at the right
                current_dirty_image = np.hstack(
                    (
                        current_dirty_image,
                        np.ones(
                            (current_dirty_image.shape[0], height - current_dirty_image.shape[1], 3),
                            dtype=np.uint8,
                        )
                        * 255,
                    )
                )
            if height != current_dirty_image.shape[1]:
                # Add white band at the bottom
                current_dirty_image = np.vstack(
                    (
                        current_dirty_image,
                        np.ones(
                            (width - current_dirty_image.shape[0], current_dirty_image.shape[1], 3),
                            dtype=np.uint8,
                        )
                        * 255,
                    )
                )

            current_dirty_images = data_augmentation_image(current_dirty_image, IMAGE_DIMENSION)
            images = data_augmentation_image(image, IMAGE_DIMENSION)
            for dirty_image, image in zip(current_dirty_images, images):
                yield dirty_image, image


def train_many(model_name: str, filenames: list[str], test: list[str]) -> None:
    if os.path.exists(f"results/{model_name}.keras"):
        model = keras.saving.load_model(f"results/{model_name}.keras")
    else:
        model = MODELS[model_name]()
    if not os.path.exists(f"results/{model_name}.summary"):
        with open(f"results/{model_name}.summary", "w", encoding="utf-8") as summary_file:
            model.summary(print_fn=lambda x: summary_file.write(x + "\n"))

    # create a tf.data.Dataset.from_generator that returns to numpy array od shape (255, 255, 3
    train = tf.data.Dataset.from_generator(
        Images(filenames),
        output_signature=(
            tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32),
            tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32),
        ),
    )
    val = tf.data.Dataset.from_generator(
        Images(test),
        output_signature=(
            tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32),
            tf.TensorSpec(shape=(None, None, None, 3), dtype=tf.float32),
        ),
    )

    try:
        if os.path.exists(f"results/{model_name}-history.yaml"):
            with open(f"results/{model_name}-history.yaml", encoding="utf-8") as history_file:
                history = yaml.load(history_file.read(), Loader=yaml.SafeLoader)
        else:
            history = {
                "loss": [],
                "val_loss": [],
            }
        hist = model.fit(
            train,
            validation_data=val,
            batch_size=100,
        )
        history.setdefault("loss", []).append(hist.history["loss"])
        history.setdefault("val_loss", []).append(hist.history["val_loss"])

        plt.plot(history["loss"])
        plt.plot(history["val_loss"])
        plt.title("Model loss")
        plt.ylabel("Loss")
        plt.legend(["Train", "Test"], loc="upper left")
        # Save plot to file
        plt.savefig(f"results/{model_name}.png")

        # Save the model
        model.save(f"results/{model_name}.keras", overwrite=True)
        with open(f"results/{model_name}-history.yaml", "w", encoding="utf-8") as history_file:
            history_file.write(yaml.dump(history))

        for test_file in test:
            print(f"Testing {test_file} => results/{model_name}-{os.path.basename(test_file)}")
            apply(model, test_file, f"results/{model_name}-{os.path.basename(test_file)}")

    except Exception as e:
        print(f"Failed to train model {model_name}: {e}")
        raise e


for model_name in MODELS:
    model = MODELS[model_name]()
    try:
        for n, i in enumerate(list(glob.glob("clean-test/*.png"))):
            apply(model, i, f"test/{model_name}-{n}.png")
    except Exception as e:
        print(f"Failed to apply model {model_name}: {e}")
